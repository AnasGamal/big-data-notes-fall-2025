{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnasGamal/big-data-notes-fall-2025/blob/main/002.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DyjMWz-1qpp",
        "outputId": "3877f8b3-89ff-4a01-8ea3-6282559b61ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# 9/8/2025\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSLn75Hx12Gn",
        "outputId": "595315e5-bdde-4c18-c958-9e24c5a62547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
            "['go jurong point crazi avail bugi n great world la e buffet cine got amor wat', 'ok lar joke wif u oni', 'free entri wkli comp win fa cup final tkt st may text fa receiv entri question std txt rate c appli', 'u dun say earli hor u c alreadi say', 'nah think goe usf live around though']\n"
          ]
        }
      ],
      "source": [
        "ps = PorterStemmer()\n",
        "corpus = []\n",
        "\n",
        "## Check if file is located locally at data/002-SMSSpamCollection.txt\n",
        "if os.path.exists('data/002-SMSSpamCollection.txt'):\n",
        "  messages = pd.read_csv('data/002-SMSSpamCollection.txt', sep='\\t', names=[\"label\",\"message\"])\n",
        "# If not, download from an online source\n",
        "else:\n",
        "  messages = pd.read_csv('https://raw.githubusercontent.com/AnasGamal/big-data-notes-fall-2025/refs/heads/main/data/002-SMSSpamCollection.txt', sep='\\t', names=[\"label\",\"message\"])\n",
        "print(messages.head())\n",
        "\n",
        "for i in range(0, len(messages)):\n",
        "    review = re.sub('[^a-zA-Z]',' ',messages['message'][i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    ### Stemming: Transforming words into their roots, [go,going] = \"go\" ---- [diner,dine,dining] = \"dine\"\n",
        "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)\n",
        "print(corpus[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mpUXVq6K2tTX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rmDR2rfq2v1d"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "'''\n",
        "CountVectorizer is a great tool provided by the scikit-learn library in Python.\n",
        "It is used to transform a given text into a vector on the basis of the frequency (count)\n",
        "of each word that occurs in the entire text.\n",
        "This is helpful when we have multiple such texts,\n",
        "and we wish to convert each word in each text into vectors (for using in further text analysis).\n",
        "\n",
        "source: https://www.geeksforgeeks.org/nlp/using-countvectorizer-to-extracting-features-from-text/\n",
        "'''\n",
        "cv = CountVectorizer(max_features=2500)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "\n",
        "y = pd.get_dummies(messages['label'])\n",
        "y = y.iloc[:,1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KcVKbEEN217W"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KaXel5jR25SC"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "'''MultinomialNB:\n",
        "In Multinomial Naive bayes the word\n",
        "\"Naive\" means that the method assumes all features - like words in a sentence - are independent from each other\n",
        "# Note: word is a feature\n",
        "\"Multinomial\" refers to how many times a word appears or how often a category occurs. It works by using word counts to classify text.\n",
        "The main idea is that it assumes each word in a message or feature is independent of each others.\n",
        "This means the presence of one word doesn't affect the presence of another word which makes the model easy to use.\n",
        "\n",
        "source: https://www.geeksforgeeks.org/machine-learning/multinomial-naive-bayes/\n",
        "'''\n",
        "spam_classifier = MultinomialNB().fit(X_train,y_train)\n",
        "\n",
        "y_pred = spam_classifier.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vQa98C52-O8",
        "outputId": "3d33cc65-5d03-48c1-8052-aa129a40aa78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9865470852017937\n",
            "[[947   8]\n",
            " [  7 153]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test,y_pred)\n",
        "accuracy = accuracy_score(y_test,y_pred)\n",
        "print(accuracy)\n",
        "print(confusion_mat)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}